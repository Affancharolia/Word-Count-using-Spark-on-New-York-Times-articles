{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXGJ4RyhyMlh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoKLtPmMyOe1"
      },
      "source": [
        "# Starting Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-urKkCUC77Ba",
        "outputId": "95539525-b509-42c4-e296-38c24ad56b1c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGYe2pN6NJYn"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q !wget -q http://apache.osuosl.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop3.2.tgz\r\n",
        "!tar xf spark-3.0.2-bin-hadoop3.2.tgz\r\n",
        "!pip install -q findspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5o6mxfaNJdM",
        "outputId": "84b0fcd0-6abd-4ff0-b1e8-7b6315b5dfa8"
      },
      "source": [
        "!pip install -U pyarrow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyarrow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/61/814f4c8d2cd4d51dfd80a9c4ea14b8fd09e37cb0f6962c1f04d504a02e03/pyarrow-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (20.7MB)\n",
            "\u001b[K     |████████████████████████████████| 20.7MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.19.5)\n",
            "Installing collected packages: pyarrow\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed pyarrow-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQCW382QNJhM"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop3.2\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs7_7G0gNOW3"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "from pyspark import SparkContext, SparkConf\r\n",
        "from pyspark.sql import SQLContext, SparkSession\r\n",
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlRp5sDANOaw"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "\t\r\n",
        "\t# create Spark context with necessary configuration\r\n",
        "\tsc = SparkContext(\"local\",\"PySpark Word Count Exmaple\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-WFOLGazOlt"
      },
      "source": [
        "# Extracting names of all the news categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ukTsQED52cB"
      },
      "source": [
        "category_set1=list()\r\n",
        "import re\r\n",
        "with open('/content/drive/My Drive/Colab Notebooks/urls.txt', 'r') as f: \r\n",
        "    for line in f:\r\n",
        "      obj2 = re.findall('://[\\w\\-\\.]+/[0-9]+/[0-9]+/[0-9]+/([\\w\\-]+)',  \r\n",
        "                  line) \r\n",
        "      str1 = ''.join(str(e) for e in obj2)\r\n",
        "      #print(str1)\r\n",
        "      category_set1.append(str1)\r\n",
        "category_set2 = set(category_set1)\r\n",
        "category_list = list(category_set2)\r\n",
        "for category_set3 in category_list:\r\n",
        "  if category_set3 == '':\r\n",
        "    category_list.remove(category_set3)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk1FJY53ELr6"
      },
      "source": [
        "check=[]\r\n",
        "for i in category_list:\r\n",
        "  category='/'+i+'/'\r\n",
        "  check.append(category)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGXB-rZWzku-"
      },
      "source": [
        "# Spliting the entire nytimes file at URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni5ok8fV52iU"
      },
      "source": [
        "noe = list()\r\n",
        "with open('/content/drive/My Drive/Colab Notebooks/nytimes.txt', 'r') as f: \r\n",
        "    x=f.read()\r\n",
        "x=x.lower()\r\n",
        "noe=x.split('url:')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj1bZSxUzzJh"
      },
      "source": [
        "# Forming the list of top 10 words from the news articles of each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy9DZ8aZGRFv",
        "outputId": "c90a2e42-dfe7-48e3-af26-4d78902db4b8"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "\r\n",
        "#For storing final output\r\n",
        "print_data = list()\r\n",
        "#For storing all the articles of a particular category\r\n",
        "category_data = list()\r\n",
        "#For storing word and count of top 10 words\r\n",
        "zz=list()\r\n",
        "gg=list()\r\n",
        "for ls in check:\r\n",
        "  category_data.clear()\r\n",
        "  print_data.clear()\r\n",
        "  print_data.append(\"Category: \"+ ls)\r\n",
        "  for words in noe:\r\n",
        "    if re.search(ls, words):  \r\n",
        "      category_data.append(words) \r\n",
        "    else:\r\n",
        "      pass\r\n",
        "  file1 = open('/content/drive/My Drive/Colab Notebooks/temp_save.txt', 'w') \r\n",
        "  for ddd in category_data:\r\n",
        "    file1.write(ddd+'\\n')\r\n",
        "  file1.close()\r\n",
        "\r\n",
        "\r\n",
        "  ####    Punctuation removal\r\n",
        "  with open('/content/drive/My Drive/Colab Notebooks/temp_save.txt', 'r') as f: \r\n",
        "    x=f.read()\r\n",
        "  x=x.lower()\r\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "  y=tokenizer.tokenize(x)\r\n",
        "  filtered_sentence = (\" \").join(y)\r\n",
        "\r\n",
        "  file1 = open('/content/drive/My Drive/Colab Notebooks/temp_save.txt', 'w') \r\n",
        "  file1.write(filtered_sentence)\r\n",
        "  file1.close()\r\n",
        "\r\n",
        "  ####    WORDCOUNT   \r\n",
        "  # Define a list of stop words or use default list\r\n",
        "  remover = StopWordsRemover()\r\n",
        "  stop_words = remover.getStopWords() \r\n",
        "  # read data from text file and split each line into words\r\n",
        "  words = sc.textFile('/content/drive/My Drive/Colab Notebooks/temp_save.txt').flatMap(lambda line: line.split(\" \"))\r\n",
        "  # Convert the words in lower case and remove stop words from stop_words\r\n",
        "  splitRDD_no_stop = words.filter(lambda x: x.lower() not in stop_words)\r\n",
        "\r\n",
        "  # Create a tuple of the word and 1 \r\n",
        "  splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\r\n",
        "\r\n",
        "  # Count of the number of occurences of each word \r\n",
        "  wordCounts = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\r\n",
        "  #wordCounts= words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)\r\n",
        "  wordCounts = wordCounts.map(lambda x:(x[1],x[0]))\r\n",
        "  wordCounts=wordCounts.sortByKey(False)\r\n",
        "  wordCounts = wordCounts.map(lambda x:(x[1],x[0]))\r\n",
        "  xy=wordCounts.take(10)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  #Forming the list of top 10 words\r\n",
        "  zz.clear()\r\n",
        "  gg.clear()\r\n",
        "  for xx in xy:\r\n",
        "    zz.append(xx[1])\r\n",
        "    gg.append(xx[0])\r\n",
        "  #Storing the top 10 words\r\n",
        "  for ee in range(10):\r\n",
        "    print_data.append(\"Word:\"+gg[ee]+\"   Count:\"+str(zz[ee]))\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "  #Forming the file\r\n",
        "  file1 = open('/content/drive/My Drive/Colab Notebooks/out_task3.txt', 'a')\r\n",
        "  for ddd in print_data:\r\n",
        "    file1.write(ddd+'\\n')\r\n",
        "  file1.write('\\n\\n\\n')\r\n",
        "  file1.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHgdMMvc2GsY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}